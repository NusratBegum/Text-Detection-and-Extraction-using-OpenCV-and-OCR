{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c06269c1",
   "metadata": {},
   "source": [
    "# Text Detection and Extraction using OpenCV and OCR\n",
    "## Handwriting Recognition Project\n",
    "\n",
    "---\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "**Business Context:**  \n",
    "This project focuses on building a Handwriting Recognition system using Computer Vision and Deep Learning techniques. Character Recognition utilizes image processing technologies to convert characters on scanned documents into digital forms. While OCR performs well with machine-printed fonts, handwritten text recognition remains challenging due to the huge variation in individual writing styles.\n",
    "\n",
    "**Dataset Description:**  \n",
    "- **Source:** Kaggle - Handwriting Recognition Dataset\n",
    "- **Size:** 400,000+ handwritten names collected through charity projects\n",
    "- **Content:** 206,799 first names and 207,024 surnames\n",
    "- **Split:** Training (331,059), Testing (41,382), Validation (41,382)\n",
    "\n",
    "---\n",
    "\n",
    "### Project Objectives\n",
    "\n",
    "1. **Primary Goal:** Build an OCR system to recognize and transcribe handwritten names\n",
    "2. **Secondary Goals:**\n",
    "   - Perform comprehensive EDA on the handwriting dataset\n",
    "   - Engineer relevant features from image data\n",
    "   - Implement text detection using OpenCV\n",
    "   - Build and evaluate deep learning models (CRNN/CNN-LSTM)\n",
    "\n",
    "---\n",
    "\n",
    "### Hypothesis\n",
    "\n",
    "**H0 (Null Hypothesis):** Deep learning models cannot effectively distinguish and transcribe handwritten text with high accuracy (>85%)\n",
    "\n",
    "**H1 (Alternative Hypothesis):** Deep learning models with proper preprocessing and feature engineering can achieve >85% accuracy in handwritten text recognition\n",
    "\n",
    "---\n",
    "\n",
    "### Success Metrics\n",
    "\n",
    "| Metric | Target | Description |\n",
    "|--------|--------|-------------|\n",
    "| Character Error Rate (CER) | < 15% | Percentage of incorrectly predicted characters |\n",
    "| Word Error Rate (WER) | < 20% | Percentage of incorrectly predicted words |\n",
    "| Training Accuracy | > 90% | Model accuracy on training set |\n",
    "| Validation Accuracy | > 85% | Model accuracy on validation set |\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Data Science Team  \n",
    "**Date:** December 2025  \n",
    "**Version:** 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea05fb8",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Environment Setup & Library Imports](#1-environment-setup)\n",
    "2. [Data Acquisition](#2-data-acquisition)\n",
    "3. [Data Loading & Initial Exploration](#3-data-loading)\n",
    "4. [Feature Types Analysis](#4-feature-types)\n",
    "5. [Exploratory Data Analysis (EDA)](#5-eda)\n",
    "6. [Feature Engineering](#6-feature-engineering)\n",
    "7. [Data Preprocessing](#7-preprocessing)\n",
    "8. [Model Architecture](#8-model-architecture)\n",
    "9. [Model Training](#9-model-training)\n",
    "10. [Model Evaluation](#10-model-evaluation)\n",
    "11. [Predictions & Inference](#11-predictions)\n",
    "12. [Conclusions & Recommendations](#12-conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b38d04",
   "metadata": {},
   "source": [
    "<a id=\"1-environment-setup\"></a>\n",
    "## 1. Environment Setup & Library Imports\n",
    "\n",
    "Installing and importing all necessary libraries for:\n",
    "- **Data Manipulation:** pandas, numpy\n",
    "- **Visualization:** matplotlib, seaborn, plotly\n",
    "- **Image Processing:** OpenCV, PIL\n",
    "- **Deep Learning:** TensorFlow/Keras\n",
    "- **Utilities:** os, glob, warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cef53c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Required Packages (if needed)\n",
    "# Uncomment the following lines if packages are not installed\n",
    "\n",
    "# !pip install kagglehub\n",
    "# !pip install opencv-python\n",
    "# !pip install tensorflow\n",
    "# !pip install plotly\n",
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c40159e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.20.0\n",
      "NumPy: 2.3.5\n",
      "Pandas: 2.3.1\n",
      "OpenCV: 4.11.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, MaxPooling2D, Dense, Dropout, Flatten,\n",
    "    Input, Reshape, BatchNormalization, Bidirectional,\n",
    "    LSTM, Lambda, Activation\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"OpenCV: {cv2.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25580ae",
   "metadata": {},
   "source": [
    "<a id=\"2-data-acquisition\"></a>\n",
    "## 2. Data Acquisition\n",
    "\n",
    "Downloading the Handwriting Recognition dataset from Kaggle using the `kagglehub` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d40db50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path: /Users/nusratbegum/.cache/kagglehub/datasets/landlord/handwriting-recognition/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"landlord/handwriting-recognition\")\n",
    "print(f\"Dataset path: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "725c5f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Directory Structure:\n",
      "  [DIR] test_v2 (1)\n",
      "  [DIR] train_v2 (1)\n",
      "  [DIR] validation_v2 (1)\n",
      "  [FILE] written_name_test_v2.csv (0.88 MB)\n",
      "  [FILE] written_name_train_v2.csv (7.65 MB)\n",
      "  [FILE] written_name_validation_v2.csv (1.12 MB)\n"
     ]
    }
   ],
   "source": [
    "def explore_directory(path):\n",
    "    \"\"\"Recursively explore directory structure.\"\"\"\n",
    "    items = []\n",
    "    for item in os.listdir(path):\n",
    "        item_path = os.path.join(path, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            items.append(('dir', item, len(os.listdir(item_path))))\n",
    "        else:\n",
    "            size = os.path.getsize(item_path) / (1024 * 1024)\n",
    "            items.append(('file', item, f\"{size:.2f} MB\"))\n",
    "    return items\n",
    "\n",
    "print(\"Dataset Directory Structure:\")\n",
    "for item_type, name, info in sorted(explore_directory(path)):\n",
    "    prefix = \"[DIR]\" if item_type == 'dir' else \"[FILE]\"\n",
    "    print(f\"  {prefix} {name} ({info})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d666652",
   "metadata": {},
   "source": [
    "<a id=\"3-data-loading\"></a>\n",
    "## 3. Data Loading & Initial Exploration\n",
    "\n",
    "Loading the CSV files containing image paths and their corresponding transcriptions (labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7585f71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loading Summary:\n",
      "  Training:   330,961 samples\n",
      "  Validation: 41,370 samples\n",
      "  Test:       41,370 samples\n",
      "  Total:      413,701 samples\n"
     ]
    }
   ],
   "source": [
    "train_csv_path = os.path.join(path, 'written_name_train_v2.csv')\n",
    "test_csv_path = os.path.join(path, 'written_name_test_v2.csv')\n",
    "val_csv_path = os.path.join(path, 'written_name_validation_v2.csv')\n",
    "\n",
    "df_train = pd.read_csv(train_csv_path)\n",
    "df_test = pd.read_csv(test_csv_path)\n",
    "df_val = pd.read_csv(val_csv_path)\n",
    "\n",
    "print(\"Data Loading Summary:\")\n",
    "print(f\"  Training:   {len(df_train):,} samples\")\n",
    "print(f\"  Validation: {len(df_val):,} samples\")\n",
    "print(f\"  Test:       {len(df_test):,} samples\")\n",
    "print(f\"  Total:      {len(df_train) + len(df_val) + len(df_test):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48675506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FILENAME</th>\n",
       "      <th>IDENTITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_00001.jpg</td>\n",
       "      <td>BALTHAZAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_00002.jpg</td>\n",
       "      <td>SIMON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_00003.jpg</td>\n",
       "      <td>BENES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_00004.jpg</td>\n",
       "      <td>LA LOVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_00005.jpg</td>\n",
       "      <td>DAPHNE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TRAIN_00006.jpg</td>\n",
       "      <td>LUCIE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TRAIN_00007.jpg</td>\n",
       "      <td>NASSIM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TRAIN_00008.jpg</td>\n",
       "      <td>ASSRAOUI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TRAIN_00009.jpg</td>\n",
       "      <td>LAVIAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TRAIN_00010.jpg</td>\n",
       "      <td>MAEVA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          FILENAME   IDENTITY\n",
       "0  TRAIN_00001.jpg  BALTHAZAR\n",
       "1  TRAIN_00002.jpg      SIMON\n",
       "2  TRAIN_00003.jpg      BENES\n",
       "3  TRAIN_00004.jpg    LA LOVE\n",
       "4  TRAIN_00005.jpg     DAPHNE\n",
       "5  TRAIN_00006.jpg      LUCIE\n",
       "6  TRAIN_00007.jpg     NASSIM\n",
       "7  TRAIN_00008.jpg   ASSRAOUI\n",
       "8  TRAIN_00009.jpg     LAVIAN\n",
       "9  TRAIN_00010.jpg      MAEVA"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns: ['FILENAME', 'IDENTITY']\n",
      "\n",
      "Data Types:\n",
      "FILENAME    object\n",
      "IDENTITY    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "display(df_train.head(10))\n",
    "print(f\"\\nColumns: {df_train.columns.tolist()}\")\n",
    "print(f\"\\nData Types:\\n{df_train.dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92c9b4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set\n",
      "--------------------------------------------------\n",
      "Shape: 330,961 rows x 2 columns\n",
      "\n",
      "Missing Values:\n",
      "  Column  Missing  Pct\n",
      "FILENAME        0 0.00\n",
      "IDENTITY      565 0.17\n",
      "\n",
      "Duplicates: 0 (0.00%)\n",
      "Memory: 37.94 MB\n",
      "\n",
      "Validation Set\n",
      "--------------------------------------------------\n",
      "Shape: 41,370 rows x 2 columns\n",
      "\n",
      "Missing Values:\n",
      "  Column  Missing  Pct\n",
      "FILENAME        0 0.00\n",
      "IDENTITY       78 0.19\n",
      "\n",
      "Duplicates: 0 (0.00%)\n",
      "Memory: 4.90 MB\n",
      "\n",
      "Test Set\n",
      "--------------------------------------------------\n",
      "Shape: 41,370 rows x 2 columns\n",
      "\n",
      "Missing Values:\n",
      "  Column  Missing  Pct\n",
      "FILENAME        0 0.00\n",
      "IDENTITY       70 0.17\n",
      "\n",
      "Duplicates: 0 (0.00%)\n",
      "Memory: 4.67 MB\n"
     ]
    }
   ],
   "source": [
    "def data_quality_report(df, name):\n",
    "    \"\"\"Generate comprehensive data quality report.\"\"\"\n",
    "    print(f\"\\n{name}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Shape: {df.shape[0]:,} rows x {df.shape[1]} columns\")\n",
    "    \n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (df.isnull().sum() / len(df) * 100).round(2)\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Column': df.columns,\n",
    "        'Missing': missing.values,\n",
    "        'Pct': missing_pct.values\n",
    "    })\n",
    "    print(f\"\\nMissing Values:\\n{missing_df.to_string(index=False)}\")\n",
    "    \n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"\\nDuplicates: {duplicates:,} ({duplicates/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    memory = df.memory_usage(deep=True).sum() / (1024**2)\n",
    "    print(f\"Memory: {memory:.2f} MB\")\n",
    "\n",
    "data_quality_report(df_train, \"Training Set\")\n",
    "data_quality_report(df_val, \"Validation Set\")\n",
    "data_quality_report(df_test, \"Test Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfbb4e9",
   "metadata": {},
   "source": [
    "<a id=\"4-feature-types\"></a>\n",
    "## 4. Feature Types Analysis\n",
    "\n",
    "Understanding the different types of features in our dataset:\n",
    "\n",
    "### Feature Categories\n",
    "\n",
    "| Feature Type | Description | Examples in Dataset |\n",
    "|-------------|-------------|---------------------|\n",
    "| **Image Features (Unstructured)** | Raw pixel data from handwritten images | Image files (JPG) |\n",
    "| **Categorical Features** | Image filename/path | `FILENAME` column |\n",
    "| **Text Features (Target)** | Transcribed handwritten text | `IDENTITY` column |\n",
    "\n",
    "### Derived Feature Types (to be engineered)\n",
    "\n",
    "| Feature Type | Description |\n",
    "|-------------|-------------|\n",
    "| **Spatial Features** | Image dimensions, aspect ratio |\n",
    "| **Intensity Features** | Pixel intensity statistics (mean, std, min, max) |\n",
    "| **Texture Features** | Edge density, stroke thickness |\n",
    "| **Morphological Features** | Character count, word boundaries |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c8e9c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Types Analysis\n",
      "--------------------------------------------------\n",
      "\n",
      "Column: FILENAME\n",
      "  Type: object, Unique: 330,961\n",
      "  Feature Type: Categorical (File Path)\n",
      "  Samples: ['TRAIN_00001.jpg', 'TRAIN_00002.jpg', 'TRAIN_00003.jpg']\n",
      "\n",
      "Column: IDENTITY\n",
      "  Type: object, Unique: 100,539\n",
      "  Feature Type: Text (Target Variable)\n",
      "  Samples: ['BALTHAZAR', 'SIMON', 'BENES']\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature Types Analysis\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for col in df_train.columns:\n",
    "    dtype = df_train[col].dtype\n",
    "    unique_count = df_train[col].nunique()\n",
    "    sample_values = df_train[col].dropna().head(3).tolist()\n",
    "    \n",
    "    if 'FILENAME' in col.upper():\n",
    "        feature_type = \"Categorical (File Path)\"\n",
    "    elif dtype == 'object' and unique_count > 100:\n",
    "        feature_type = \"Text (Target Variable)\"\n",
    "    else:\n",
    "        feature_type = \"Unknown\"\n",
    "    \n",
    "    print(f\"\\nColumn: {col}\")\n",
    "    print(f\"  Type: {dtype}, Unique: {unique_count:,}\")\n",
    "    print(f\"  Feature Type: {feature_type}\")\n",
    "    print(f\"  Samples: {sample_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192ca43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Directories\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_images_dir = os.path.join(path, 'train_v2', 'train')\n",
    "test_images_dir = os.path.join(path, 'test_v2', 'test')\n",
    "val_images_dir = os.path.join(path, 'validation_v2', 'validation')\n",
    "\n",
    "print(\"Image Directories\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for name, dir_path in [('Training', train_images_dir), \n",
    "                        ('Validation', val_images_dir), \n",
    "                        ('Test', test_images_dir)]:\n",
    "    if os.path.exists(dir_path):\n",
    "        num_images = len([f for f in os.listdir(dir_path) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        print(f\"{name}: {num_images:,} images\")\n",
    "    else:\n",
    "        print(f\"{name}: Directory not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4434031a",
   "metadata": {},
   "source": [
    "<a id=\"5-eda\"></a>\n",
    "## 5. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Comprehensive analysis of the dataset to understand patterns, distributions, and characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8454e651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and prepare the IDENTITY column\n",
    "df_train_clean = df_train.dropna(subset=['IDENTITY']).copy()\n",
    "df_train_clean['IDENTITY'] = df_train_clean['IDENTITY'].astype(str)\n",
    "df_train_clean = df_train_clean[df_train_clean['IDENTITY'] != 'UNREADABLE']\n",
    "\n",
    "print(\"Text Label Statistics\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total samples: {len(df_train_clean):,}\")\n",
    "print(f\"Unique labels: {df_train_clean['IDENTITY'].nunique():,}\")\n",
    "\n",
    "df_train_clean['label_length'] = df_train_clean['IDENTITY'].str.len()\n",
    "print(f\"\\nLabel Length Statistics:\")\n",
    "print(df_train_clean['label_length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d46f870",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(df_train_clean['label_length'], bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Label Length (characters)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Label Lengths', fontweight='bold')\n",
    "axes[0].axvline(df_train_clean['label_length'].mean(), color='red', linestyle='--', \n",
    "                label=f'Mean: {df_train_clean[\"label_length\"].mean():.1f}')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].boxplot(df_train_clean['label_length'], vert=True)\n",
    "axes[1].set_ylabel('Label Length (characters)')\n",
    "axes[1].set_title('Label Length Box Plot', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea9387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = ''.join(df_train_clean['IDENTITY'].values)\n",
    "char_freq = Counter(all_text.lower())\n",
    "\n",
    "char_df = pd.DataFrame(list(char_freq.items()), columns=['Character', 'Frequency'])\n",
    "char_df = char_df.sort_values('Frequency', ascending=False).head(30)\n",
    "\n",
    "fig = px.bar(char_df, x='Character', y='Frequency', \n",
    "             title='Top 30 Most Frequent Characters',\n",
    "             color='Frequency', color_continuous_scale='viridis')\n",
    "fig.update_layout(xaxis_title='Character', yaxis_title='Frequency', height=500, showlegend=False)\n",
    "fig.show()\n",
    "\n",
    "print(f\"Total unique characters: {len(char_freq)}\")\n",
    "print(f\"Most common: '{char_df.iloc[0]['Character']}' ({char_df.iloc[0]['Frequency']:,} occurrences)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5d7ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_counts = df_train_clean['IDENTITY'].value_counts().head(20)\n",
    "\n",
    "fig = px.bar(x=name_counts.index, y=name_counts.values,\n",
    "             title='Top 20 Most Common Names',\n",
    "             labels={'x': 'Name', 'y': 'Count'},\n",
    "             color=name_counts.values, color_continuous_scale='plasma')\n",
    "fig.update_layout(height=500, showlegend=False)\n",
    "fig.show()\n",
    "\n",
    "print(f\"Most common: '{name_counts.index[0]}' ({name_counts.values[0]:,} occurrences)\")\n",
    "print(f\"Top 10 names: {name_counts.head(10).sum():,} samples ({name_counts.head(10).sum()/len(df_train_clean)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1546b642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_display_samples(df, images_dir, n_samples=12, title=\"Sample Images\"):\n",
    "    \"\"\"Load and display sample images with their labels.\"\"\"\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "    samples = df.sample(n=min(n_samples, len(df)), random_state=42)\n",
    "    \n",
    "    for idx, (_, row) in enumerate(samples.iterrows()):\n",
    "        if idx >= n_samples:\n",
    "            break\n",
    "        img_path = os.path.join(images_dir, row['FILENAME'])\n",
    "        \n",
    "        if os.path.exists(img_path):\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                axes[idx].imshow(img)\n",
    "                axes[idx].set_title(f\"Label: {row['IDENTITY']}\", fontsize=10, fontweight='bold')\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "load_and_display_samples(df_train_clean, train_images_dir, n_samples=12, \n",
    "                         title=\"Sample Handwritten Names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfaa359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image_dimensions(df, images_dir, sample_size=1000):\n",
    "    \"\"\"Analyze image dimensions from a sample.\"\"\"\n",
    "    widths, heights, aspect_ratios = [], [], []\n",
    "    samples = df.sample(n=min(sample_size, len(df)), random_state=42)\n",
    "    \n",
    "    for _, row in samples.iterrows():\n",
    "        img_path = os.path.join(images_dir, row['FILENAME'])\n",
    "        if os.path.exists(img_path):\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                h, w = img.shape[:2]\n",
    "                widths.append(w)\n",
    "                heights.append(h)\n",
    "                aspect_ratios.append(w / h)\n",
    "    \n",
    "    return widths, heights, aspect_ratios\n",
    "\n",
    "widths, heights, aspect_ratios = analyze_image_dimensions(df_train_clean, train_images_dir)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "axes[0].hist(widths, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Width (pixels)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Image Width Distribution', fontweight='bold')\n",
    "\n",
    "axes[1].hist(heights, bins=50, color='forestgreen', edgecolor='black', alpha=0.7)\n",
    "axes[1].set_xlabel('Height (pixels)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Image Height Distribution', fontweight='bold')\n",
    "\n",
    "axes[2].hist(aspect_ratios, bins=50, color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[2].set_xlabel('Aspect Ratio (W/H)')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].set_title('Aspect Ratio Distribution', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Width  - Mean: {np.mean(widths):.0f}, Range: [{min(widths)}, {max(widths)}]\")\n",
    "print(f\"Height - Mean: {np.mean(heights):.0f}, Range: [{min(heights)}, {max(heights)}]\")\n",
    "print(f\"Aspect Ratio - Mean: {np.mean(aspect_ratios):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3560ef12",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data = {\n",
    "    'Split': ['Training', 'Validation', 'Test'],\n",
    "    'Count': [len(df_train), len(df_val), len(df_test)]\n",
    "}\n",
    "split_df = pd.DataFrame(split_data)\n",
    "split_df['Percentage'] = (split_df['Count'] / split_df['Count'].sum() * 100).round(2)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, \n",
    "                    specs=[[{\"type\": \"pie\"}, {\"type\": \"bar\"}]],\n",
    "                    subplot_titles=('Split Distribution', 'Sample Counts'))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Pie(labels=split_df['Split'], values=split_df['Count'], \n",
    "           hole=0.4, marker_colors=['#3498db', '#2ecc71', '#e74c3c']),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=split_df['Split'], y=split_df['Count'], \n",
    "           marker_color=['#3498db', '#2ecc71', '#e74c3c'],\n",
    "           text=split_df['Count'].apply(lambda x: f'{x:,}'), textposition='auto'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=450, title_text='Dataset Split Analysis', showlegend=False)\n",
    "fig.show()\n",
    "\n",
    "print(split_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10be46c0",
   "metadata": {},
   "source": [
    "<a id=\"6-feature-engineering\"></a>\n",
    "## 6. Feature Engineering\n",
    "\n",
    "Creating new features from existing data to improve model performance.\n",
    "\n",
    "### Feature Engineering Categories\n",
    "\n",
    "1. **Image-based Features:**\n",
    "   - Standardized dimensions (resize to fixed size)\n",
    "   - Grayscale conversion\n",
    "   - Normalization (0-1 scaling)\n",
    "   \n",
    "2. **Label-based Features:**\n",
    "   - Character vocabulary/alphabet\n",
    "   - Label encoding (CTC compatible)\n",
    "   - Maximum label length determination\n",
    "\n",
    "3. **Data Augmentation Features:**\n",
    "   - Rotation\n",
    "   - Width/Height shifts\n",
    "   - Zoom variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252e35cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 64\n",
    "MAX_LABEL_LENGTH = 32\n",
    "\n",
    "# Character set (vocabulary)\n",
    "CHAR_LIST = string.ascii_letters + string.digits + \" '-.\"\n",
    "\n",
    "# Character mappings (0 reserved for CTC blank)\n",
    "char_to_num = {char: idx + 1 for idx, char in enumerate(CHAR_LIST)}\n",
    "num_to_char = {idx + 1: char for idx, char in enumerate(CHAR_LIST)}\n",
    "num_to_char[0] = ''\n",
    "\n",
    "VOCAB_SIZE = len(CHAR_LIST) + 1\n",
    "\n",
    "print(\"Model Parameters\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Image size: {IMG_WIDTH} x {IMG_HEIGHT}\")\n",
    "print(f\"Max label length: {MAX_LABEL_LENGTH}\")\n",
    "print(f\"Character set size: {len(CHAR_LIST)}\")\n",
    "print(f\"Vocabulary size (with blank): {VOCAB_SIZE}\")\n",
    "print(f\"Characters: {CHAR_LIST}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344c4d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(df, max_length=MAX_LABEL_LENGTH, char_set=CHAR_LIST):\n",
    "    \"\"\"\n",
    "    Clean dataset by removing invalid entries and filtering by character set.\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    initial_count = len(df_clean)\n",
    "    \n",
    "    df_clean = df_clean.dropna(subset=['IDENTITY', 'FILENAME'])\n",
    "    df_clean['IDENTITY'] = df_clean['IDENTITY'].astype(str)\n",
    "    df_clean = df_clean[df_clean['IDENTITY'] != 'UNREADABLE']\n",
    "    df_clean = df_clean[df_clean['IDENTITY'].str.len() <= max_length]\n",
    "    df_clean = df_clean[df_clean['IDENTITY'].str.len() > 0]\n",
    "    \n",
    "    valid_chars_pattern = f'^[{char_set}]+$'\n",
    "    df_clean = df_clean[df_clean['IDENTITY'].str.match(valid_chars_pattern, na=False)]\n",
    "    \n",
    "    final_count = len(df_clean)\n",
    "    print(f\"  {initial_count:,} -> {final_count:,} ({final_count/initial_count*100:.1f}% retained)\")\n",
    "    \n",
    "    return df_clean.reset_index(drop=True)\n",
    "\n",
    "print(\"Data Cleaning\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Training Set:\")\n",
    "df_train_clean = clean_dataset(df_train)\n",
    "print(\"Validation Set:\")\n",
    "df_val_clean = clean_dataset(df_val)\n",
    "print(\"Test Set:\")\n",
    "df_test_clean = clean_dataset(df_test)\n",
    "\n",
    "print(f\"\\nFinal counts: Train={len(df_train_clean):,}, Val={len(df_val_clean):,}, Test={len(df_test_clean):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2024d34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label(label):\n",
    "    \"\"\"Convert text label to numeric sequence.\"\"\"\n",
    "    return [char_to_num[char] for char in label if char in char_to_num]\n",
    "\n",
    "def decode_label(encoded):\n",
    "    \"\"\"Convert numeric sequence back to text.\"\"\"\n",
    "    return ''.join(num_to_char.get(num, '') for num in encoded)\n",
    "\n",
    "def pad_label(label, max_length=MAX_LABEL_LENGTH):\n",
    "    \"\"\"Pad label to fixed length.\"\"\"\n",
    "    label = label[:max_length]\n",
    "    return label + [0] * (max_length - len(label))\n",
    "\n",
    "# Test encoding/decoding\n",
    "test_text = \"John Smith\"\n",
    "encoded = encode_label(test_text)\n",
    "decoded = decode_label(encoded)\n",
    "\n",
    "print(\"Label Encoding Test\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Original: '{test_text}'\")\n",
    "print(f\"Encoded:  {encoded}\")\n",
    "print(f\"Decoded:  '{decoded}'\")\n",
    "print(f\"Match: {test_text == decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16e0217",
   "metadata": {},
   "source": [
    "<a id=\"7-preprocessing\"></a>\n",
    "## 7. Data Preprocessing\n",
    "\n",
    "Implementing image preprocessing pipeline using OpenCV:\n",
    "1. Loading images\n",
    "2. Grayscale conversion\n",
    "3. Resizing to fixed dimensions\n",
    "4. Normalization\n",
    "5. Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab61a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img_path, target_width=IMG_WIDTH, target_height=IMG_HEIGHT):\n",
    "    \"\"\"\n",
    "    Preprocess image: grayscale, resize with aspect ratio, pad, normalize.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        return None\n",
    "    \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    h, w = img.shape\n",
    "    \n",
    "    # Resize maintaining aspect ratio\n",
    "    aspect_ratio = w / h\n",
    "    new_width = int(target_height * aspect_ratio)\n",
    "    \n",
    "    if new_width > target_width:\n",
    "        new_width = target_width\n",
    "        new_height = int(target_width / aspect_ratio)\n",
    "    else:\n",
    "        new_height = target_height\n",
    "    \n",
    "    img_resized = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # Center on white canvas\n",
    "    canvas = np.ones((target_height, target_width), dtype=np.uint8) * 255\n",
    "    x_offset = (target_width - new_width) // 2\n",
    "    y_offset = (target_height - new_height) // 2\n",
    "    canvas[y_offset:y_offset + new_height, x_offset:x_offset + new_width] = img_resized\n",
    "    \n",
    "    # Normalize and expand dims\n",
    "    img_normalized = canvas.astype(np.float32) / 255.0\n",
    "    return np.expand_dims(img_normalized, axis=-1)\n",
    "\n",
    "# Test preprocessing\n",
    "sample_filename = df_train_clean['FILENAME'].iloc[0]\n",
    "sample_path = os.path.join(train_images_dir, sample_filename)\n",
    "\n",
    "if os.path.exists(sample_path):\n",
    "    preprocessed = preprocess_image(sample_path)\n",
    "    print(f\"Input: {sample_path}\")\n",
    "    print(f\"Output shape: {preprocessed.shape}\")\n",
    "    print(f\"Value range: [{preprocessed.min():.4f}, {preprocessed.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c543d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_preprocessing(img_path, target_width=IMG_WIDTH, target_height=IMG_HEIGHT):\n",
    "    \"\"\"Visualize each step of image preprocessing.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    \n",
    "    img_original = cv2.imread(img_path)\n",
    "    img_rgb = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)\n",
    "    axes[0, 0].imshow(img_rgb)\n",
    "    axes[0, 0].set_title('1. Original', fontweight='bold')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    img_gray = cv2.cvtColor(img_original, cv2.COLOR_BGR2GRAY)\n",
    "    axes[0, 1].imshow(img_gray, cmap='gray')\n",
    "    axes[0, 1].set_title('2. Grayscale', fontweight='bold')\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    _, img_thresh = cv2.threshold(img_gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    axes[0, 2].imshow(img_thresh, cmap='gray')\n",
    "    axes[0, 2].set_title('3. Otsu Threshold', fontweight='bold')\n",
    "    axes[0, 2].axis('off')\n",
    "    \n",
    "    h, w = img_gray.shape\n",
    "    aspect_ratio = w / h\n",
    "    new_width = int(target_height * aspect_ratio)\n",
    "    if new_width > target_width:\n",
    "        new_width = target_width\n",
    "        new_height = int(target_width / aspect_ratio)\n",
    "    else:\n",
    "        new_height = target_height\n",
    "    img_resized = cv2.resize(img_gray, (new_width, new_height))\n",
    "    axes[1, 0].imshow(img_resized, cmap='gray')\n",
    "    axes[1, 0].set_title(f'4. Resized ({new_width}x{new_height})', fontweight='bold')\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    canvas = np.ones((target_height, target_width), dtype=np.uint8) * 255\n",
    "    x_offset = (target_width - new_width) // 2\n",
    "    y_offset = (target_height - new_height) // 2\n",
    "    canvas[y_offset:y_offset + new_height, x_offset:x_offset + new_width] = img_resized\n",
    "    axes[1, 1].imshow(canvas, cmap='gray')\n",
    "    axes[1, 1].set_title(f'5. Padded ({target_width}x{target_height})', fontweight='bold')\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    img_normalized = canvas.astype(np.float32) / 255.0\n",
    "    axes[1, 2].imshow(img_normalized, cmap='gray')\n",
    "    axes[1, 2].set_title('6. Normalized [0,1]', fontweight='bold')\n",
    "    axes[1, 2].axis('off')\n",
    "    \n",
    "    plt.suptitle('Preprocessing Pipeline', fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_preprocessing(sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ca0dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ultra-minimal for CPU demo\n",
    "TRAIN_SIZE = 50\n",
    "VAL_SIZE = 20\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "df_train_sample = df_train_clean.head(TRAIN_SIZE).reset_index(drop=True)\n",
    "df_val_sample = df_val_clean.head(VAL_SIZE).reset_index(drop=True)\n",
    "\n",
    "print(f\"Using {TRAIN_SIZE} training samples and {VAL_SIZE} validation samples for quick demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a451e5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandwritingDataGenerator(keras.utils.Sequence):\n",
    "    \"\"\"Custom data generator for handwriting recognition.\"\"\"\n",
    "    \n",
    "    def __init__(self, df, images_dir, batch_size=32, img_width=IMG_WIDTH, \n",
    "                 img_height=IMG_HEIGHT, max_label_len=MAX_LABEL_LENGTH, shuffle=True):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.images_dir = images_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.img_width = img_width\n",
    "        self.img_height = img_height\n",
    "        self.max_label_len = max_label_len\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.df))\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.df) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        start_idx = index * self.batch_size\n",
    "        end_idx = min((index + 1) * self.batch_size, len(self.df))\n",
    "        batch_indices = self.indices[start_idx:end_idx]\n",
    "        \n",
    "        batch_size = len(batch_indices)\n",
    "        images = np.zeros((batch_size, self.img_height, self.img_width, 1), dtype=np.float32)\n",
    "        labels = np.zeros((batch_size, self.max_label_len), dtype=np.int32)\n",
    "        input_length = np.zeros((batch_size, 1), dtype=np.int32)\n",
    "        label_length = np.zeros((batch_size, 1), dtype=np.int32)\n",
    "        \n",
    "        for i, idx in enumerate(batch_indices):\n",
    "            row = self.df.iloc[idx]\n",
    "            img_path = os.path.join(self.images_dir, row['FILENAME'])\n",
    "            img = preprocess_image(img_path, self.img_width, self.img_height)\n",
    "            \n",
    "            if img is not None:\n",
    "                images[i] = img\n",
    "            \n",
    "            label_text = str(row['IDENTITY'])\n",
    "            encoded = encode_label(label_text)[:self.max_label_len]\n",
    "            labels[i, :len(encoded)] = encoded\n",
    "            input_length[i] = self.img_width // 4 - 2\n",
    "            label_length[i] = len(encoded)\n",
    "        \n",
    "        inputs = {\n",
    "            'input_image': images,\n",
    "            'input_label': labels,\n",
    "            'input_length': input_length,\n",
    "            'label_length': label_length\n",
    "        }\n",
    "        outputs = {'ctc_loss': np.zeros((batch_size,))}\n",
    "        return inputs, outputs\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "train_generator = HandwritingDataGenerator(df_train_sample, train_images_dir, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_generator = HandwritingDataGenerator(df_val_sample, val_images_dir, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Training batches: {len(train_generator)}\")\n",
    "print(f\"Validation batches: {len(val_generator)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8169a48",
   "metadata": {},
   "source": [
    "<a id=\"8-model-architecture\"></a>\n",
    "## 8. Model Architecture\n",
    "\n",
    "Building a CRNN (Convolutional Recurrent Neural Network) model for handwriting recognition.\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "```\n",
    "Input (64, 256, 1)\n",
    "        |\n",
    "        v\n",
    "+---------------------+\n",
    "|  CNN Feature        |  - Extract visual features\n",
    "|  Extractor          |  - Multiple Conv + Pool layers\n",
    "+---------------------+\n",
    "        |\n",
    "        v\n",
    "+---------------------+\n",
    "|  Reshape Layer      |  - Prepare for RNN\n",
    "+---------------------+\n",
    "        |\n",
    "        v\n",
    "+---------------------+\n",
    "|  Bidirectional      |  - Capture sequential patterns\n",
    "|  LSTM Layers        |\n",
    "+---------------------+\n",
    "        |\n",
    "        v\n",
    "+---------------------+\n",
    "|  Dense + Softmax    |  - Character classification\n",
    "+---------------------+\n",
    "        |\n",
    "        v\n",
    "+---------------------+\n",
    "|  CTC Loss           |  - Sequence alignment\n",
    "+---------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c6a1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTCLayer(layers.Layer):\n",
    "    \"\"\"Custom layer to compute CTC loss.\"\"\"\n",
    "    \n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.loss_fn = keras.backend.ctc_batch_cost\n",
    "        \n",
    "    def call(self, y_true, y_pred, input_length, label_length):\n",
    "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
    "        self.add_loss(loss)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc5d72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_crnn_model(img_width=IMG_WIDTH, img_height=IMG_HEIGHT, vocab_size=VOCAB_SIZE):\n",
    "    \"\"\"Build lightweight CNN model for CPU training.\"\"\"\n",
    "    \n",
    "    input_image = Input(shape=(img_height, img_width, 1), name='input_image', dtype='float32')\n",
    "    input_label = Input(shape=(MAX_LABEL_LENGTH,), name='input_label', dtype='int32')\n",
    "    input_length = Input(shape=(1,), name='input_length', dtype='int32')\n",
    "    label_length = Input(shape=(1,), name='label_length', dtype='int32')\n",
    "    \n",
    "    # Lightweight CNN (reduced filters)\n",
    "    x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_image)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 1))(x)\n",
    "    \n",
    "    # Reshape for sequence output\n",
    "    new_shape = (img_width // 4, (img_height // 8) * 64)\n",
    "    x = Reshape(target_shape=new_shape, name='reshape')(x)\n",
    "    \n",
    "    # Simple Dense layers instead of LSTM (much faster on CPU)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    output = Dense(vocab_size, activation='softmax', name='output_dense')(x)\n",
    "    ctc_loss = CTCLayer(name='ctc_loss')(input_label, output, input_length, label_length)\n",
    "    \n",
    "    model = Model(\n",
    "        inputs=[input_image, input_label, input_length, label_length],\n",
    "        outputs=ctc_loss,\n",
    "        name='CNN_OCR_Light'\n",
    "    )\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001))\n",
    "    return model\n",
    "\n",
    "model = build_crnn_model()\n",
    "print(f\"Lightweight model: {model.count_params():,} parameters\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718c6305",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from tensorflow.keras.utils import plot_model\n",
    "    plot_model(model, to_file='model_architecture.png', show_shapes=True, show_layer_names=True, dpi=100)\n",
    "    from IPython.display import Image, display\n",
    "    display(Image('model_architecture.png'))\n",
    "except Exception as e:\n",
    "    print(f\"Could not generate visualization (requires graphviz): {e}\")\n",
    "\n",
    "print(\"\\nLayer Details\")\n",
    "print(\"-\" * 70)\n",
    "for layer in model.layers:\n",
    "    try:\n",
    "        output_shape = str(layer.output.shape) if hasattr(layer, 'output') else 'N/A'\n",
    "    except:\n",
    "        output_shape = 'N/A'\n",
    "    print(f\"{layer.name:25} | {output_shape:25} | {layer.count_params():,} params\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Total: {model.count_params():,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18a22f5",
   "metadata": {},
   "source": [
    "<a id=\"9-model-training\"></a>\n",
    "## 9. Model Training\n",
    "\n",
    "Training the CRNN model with early stopping, learning rate reduction, and model checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f59847",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath='best_model.keras',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks_list = [early_stopping, reduce_lr, model_checkpoint]\n",
    "\n",
    "print(\"Callbacks configured:\")\n",
    "print(\"  - Early stopping (patience=3)\")\n",
    "print(\"  - Learning rate reduction (factor=0.5, patience=2)\")\n",
    "print(\"  - Model checkpoint (best model saved)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a60e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preload all data into memory (fastest approach)\n",
    "print(\"Loading images into memory...\")\n",
    "\n",
    "X_train, y_train, train_input_len, train_label_len = [], [], [], []\n",
    "\n",
    "for idx, row in df_train_sample.iterrows():\n",
    "    img_path = os.path.join(train_images_dir, row['FILENAME'])\n",
    "    img = preprocess_image(img_path)\n",
    "    if img is not None:\n",
    "        X_train.append(img)\n",
    "        encoded = encode_label(str(row['IDENTITY']))[:MAX_LABEL_LENGTH]\n",
    "        padded = encoded + [0] * (MAX_LABEL_LENGTH - len(encoded))\n",
    "        y_train.append(padded)\n",
    "        train_input_len.append(IMG_WIDTH // 4 - 2)\n",
    "        train_label_len.append(len(encoded))\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "train_input_len = np.array(train_input_len).reshape(-1, 1)\n",
    "train_label_len = np.array(train_label_len).reshape(-1, 1)\n",
    "\n",
    "print(f\"Loaded {len(X_train)} training images\")\n",
    "\n",
    "# Quick training with preloaded data\n",
    "print(\"\\nStarting training...\")\n",
    "\n",
    "history = model.fit(\n",
    "    x={'input_image': X_train, 'input_label': y_train, \n",
    "       'input_length': train_input_len, 'label_length': train_label_len},\n",
    "    y={'ctc_loss': np.zeros(len(X_train))},\n",
    "    batch_size=16,\n",
    "    epochs=2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b53be7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training and validation loss curves.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    axes[0].plot(history.history['loss'], label='Training', color='blue', linewidth=2)\n",
    "    axes[0].plot(history.history['val_loss'], label='Validation', color='red', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training vs Validation Loss', fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    train_loss = history.history['loss']\n",
    "    loss_reduction = [(train_loss[0] - train_loss[i]) / train_loss[0] * 100 for i in range(len(train_loss))]\n",
    "    axes[1].bar(range(1, len(train_loss) + 1), loss_reduction, color='steelblue', alpha=0.7)\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Loss Reduction (%)')\n",
    "    axes[1].set_title('Training Loss Improvement', fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    val_loss = history.history['val_loss']\n",
    "    print(f\"Final Training Loss:   {train_loss[-1]:.4f}\")\n",
    "    print(f\"Final Validation Loss: {val_loss[-1]:.4f}\")\n",
    "    print(f\"Best Validation Loss:  {min(val_loss):.4f} (Epoch {val_loss.index(min(val_loss)) + 1})\")\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2ed398",
   "metadata": {},
   "source": [
    "<a id=\"10-model-evaluation\"></a>\n",
    "## 10. Model Evaluation\n",
    "\n",
    "Evaluating model performance using Character Error Rate (CER), Word Error Rate (WER), and sample predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b79802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prediction_model(training_model):\n",
    "    \"\"\"Create prediction model without CTC loss.\"\"\"\n",
    "    return Model(\n",
    "        inputs=training_model.get_layer('input_image').input,\n",
    "        outputs=training_model.get_layer('output_dense').output\n",
    "    )\n",
    "\n",
    "pred_model = build_prediction_model(model)\n",
    "print(f\"Prediction model - Input: {pred_model.input_shape}, Output: {pred_model.output_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b5c1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_decode(predictions):\n",
    "    \"\"\"Decode CTC predictions using greedy decoding.\"\"\"\n",
    "    input_len = np.ones(predictions.shape[0]) * predictions.shape[1]\n",
    "    decoded, _ = tf.keras.backend.ctc_decode(predictions, input_length=input_len, greedy=True)\n",
    "    decoded_dense = decoded[0].numpy()\n",
    "    \n",
    "    texts = []\n",
    "    for seq in decoded_dense:\n",
    "        text = ''.join(num_to_char.get(idx, '') for idx in seq if 0 <= idx < VOCAB_SIZE)\n",
    "        texts.append(text)\n",
    "    return texts\n",
    "\n",
    "def predict_text(model, img_path):\n",
    "    \"\"\"Predict text from a single image.\"\"\"\n",
    "    img = preprocess_image(img_path)\n",
    "    if img is None:\n",
    "        return None\n",
    "    \n",
    "    img_batch = np.expand_dims(img, axis=0)\n",
    "    predictions = model.predict(img_batch, verbose=0)\n",
    "    texts = ctc_decode(predictions)\n",
    "    return texts[0] if texts else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f18143e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_distance(s1, s2):\n",
    "    \"\"\"Calculate edit distance between two strings.\"\"\"\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein_distance(s2, s1)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "    \n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    return previous_row[-1]\n",
    "\n",
    "def calculate_cer(true_labels, predicted_labels):\n",
    "    \"\"\"Calculate Character Error Rate.\"\"\"\n",
    "    total_chars = sum(len(t) for t in true_labels)\n",
    "    total_errors = sum(levenshtein_distance(t, p) for t, p in zip(true_labels, predicted_labels))\n",
    "    return total_errors / total_chars if total_chars > 0 else 0\n",
    "\n",
    "def calculate_wer(true_labels, predicted_labels):\n",
    "    \"\"\"Calculate Word Error Rate.\"\"\"\n",
    "    errors = sum(1 for t, p in zip(true_labels, predicted_labels) if t.lower() != p.lower())\n",
    "    return errors / len(true_labels) if true_labels else 0\n",
    "\n",
    "def calculate_accuracy(true_labels, predicted_labels):\n",
    "    \"\"\"Calculate exact match accuracy.\"\"\"\n",
    "    correct = sum(1 for t, p in zip(true_labels, predicted_labels) if t.lower() == p.lower())\n",
    "    return correct / len(true_labels) if true_labels else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a445c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, df, images_dir, n_samples=500):\n",
    "    \"\"\"Evaluate model on a subset of data.\"\"\"\n",
    "    eval_df = df.sample(n=min(n_samples, len(df)), random_state=42)\n",
    "    true_labels, predicted_labels = [], []\n",
    "    \n",
    "    for idx, (_, row) in enumerate(eval_df.iterrows()):\n",
    "        if (idx + 1) % 100 == 0:\n",
    "            print(f\"  Processed {idx + 1}/{len(eval_df)}...\")\n",
    "        \n",
    "        img_path = os.path.join(images_dir, row['FILENAME'])\n",
    "        if os.path.exists(img_path):\n",
    "            predicted = predict_text(model, img_path)\n",
    "            if predicted is not None:\n",
    "                true_labels.append(str(row['IDENTITY']))\n",
    "                predicted_labels.append(predicted)\n",
    "    \n",
    "    return {\n",
    "        'cer': calculate_cer(true_labels, predicted_labels),\n",
    "        'wer': calculate_wer(true_labels, predicted_labels),\n",
    "        'accuracy': calculate_accuracy(true_labels, predicted_labels),\n",
    "        'true_labels': true_labels,\n",
    "        'predicted_labels': predicted_labels\n",
    "    }\n",
    "\n",
    "print(\"Evaluating on validation set...\")\n",
    "results = evaluate_model(pred_model, df_val_clean, val_images_dir, n_samples=500)\n",
    "\n",
    "print(\"\\nEvaluation Results\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Character Error Rate (CER): {results['cer']*100:.2f}%\")\n",
    "print(f\"Word Error Rate (WER):      {results['wer']*100:.2f}%\")\n",
    "print(f\"Exact Match Accuracy:       {results['accuracy']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbfbe28",
   "metadata": {},
   "source": [
    "<a id=\"11-predictions\"></a>\n",
    "## 11. Predictions & Inference\n",
    "\n",
    "Demonstrating model predictions on sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bc0a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, df, images_dir, n_samples=12):\n",
    "    \"\"\"Visualize predictions vs ground truth.\"\"\"\n",
    "    fig, axes = plt.subplots(4, 3, figsize=(15, 16))\n",
    "    axes = axes.flatten()\n",
    "    samples = df.sample(n=min(n_samples, len(df)), random_state=42)\n",
    "    \n",
    "    for idx, (_, row) in enumerate(samples.iterrows()):\n",
    "        if idx >= n_samples:\n",
    "            break\n",
    "        \n",
    "        img_path = os.path.join(images_dir, row['FILENAME'])\n",
    "        true_label = str(row['IDENTITY'])\n",
    "        \n",
    "        if os.path.exists(img_path):\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            axes[idx].imshow(img)\n",
    "            \n",
    "            predicted = predict_text(model, img_path)\n",
    "            match = true_label.lower() == predicted.lower()\n",
    "            color = 'green' if match else 'red'\n",
    "            symbol = '[OK]' if match else '[X]'\n",
    "            \n",
    "            axes[idx].set_title(f\"True: {true_label}\\nPred: {predicted} {symbol}\",\n",
    "                               fontsize=10, fontweight='bold', color=color)\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.suptitle('Predictions vs Ground Truth', fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_predictions(pred_model, df_val_clean, val_images_dir, n_samples=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ef4b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_prediction_analysis(true_labels, predicted_labels, n_show=20):\n",
    "    \"\"\"Show detailed comparison of predictions.\"\"\"\n",
    "    print(f\"{'#':>3} | {'True Label':<20} | {'Predicted':<20} | {'Match':^6} | {'Edit Dist':>9}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for i in range(min(n_show, len(true_labels))):\n",
    "        true, pred = true_labels[i], predicted_labels[i]\n",
    "        match = '[OK]' if true.lower() == pred.lower() else '[X]'\n",
    "        edit_dist = levenshtein_distance(true, pred)\n",
    "        print(f\"{i+1:>3} | {true:<20} | {pred:<20} | {match:^6} | {edit_dist:>9}\")\n",
    "    \n",
    "    correct = sum(1 for t, p in zip(true_labels, predicted_labels) if t.lower() == p.lower())\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"Summary: {correct}/{len(true_labels)} correct ({correct/len(true_labels)*100:.1f}%)\")\n",
    "\n",
    "detailed_prediction_analysis(results['true_labels'], results['predicted_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36502e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opencv_text_detection(img_path):\n",
    "    \"\"\"Demonstrate OpenCV-based text detection and preprocessing.\"\"\"\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        return None\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    axes[0, 0].imshow(img_rgb)\n",
    "    axes[0, 0].set_title('1. Original', fontweight='bold')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    axes[0, 1].imshow(gray, cmap='gray')\n",
    "    axes[0, 1].set_title('2. Grayscale', fontweight='bold')\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    axes[0, 2].imshow(blurred, cmap='gray')\n",
    "    axes[0, 2].set_title('3. Gaussian Blur', fontweight='bold')\n",
    "    axes[0, 2].axis('off')\n",
    "    \n",
    "    thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                    cv2.THRESH_BINARY_INV, 11, 2)\n",
    "    axes[1, 0].imshow(thresh, cmap='gray')\n",
    "    axes[1, 0].set_title('4. Adaptive Threshold', fontweight='bold')\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "    dilated = cv2.dilate(thresh, kernel, iterations=1)\n",
    "    axes[1, 1].imshow(dilated, cmap='gray')\n",
    "    axes[1, 1].set_title('5. Dilation', fontweight='bold')\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    contours, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    img_contours = img_rgb.copy()\n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        if w > 10 and h > 10:\n",
    "            cv2.rectangle(img_contours, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    \n",
    "    axes[1, 2].imshow(img_contours)\n",
    "    axes[1, 2].set_title('6. Detected Regions', fontweight='bold')\n",
    "    axes[1, 2].axis('off')\n",
    "    \n",
    "    plt.suptitle('OpenCV Text Detection Pipeline', fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(f\"Found {len(contours)} potential text regions\")\n",
    "\n",
    "sample_img_path = os.path.join(val_images_dir, df_val_clean['FILENAME'].iloc[0])\n",
    "if os.path.exists(sample_img_path):\n",
    "    opencv_text_detection(sample_img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d49aee",
   "metadata": {},
   "source": [
    "<a id=\"12-conclusions\"></a>\n",
    "## 12. Conclusions & Recommendations\n",
    "\n",
    "### Project Summary\n",
    "\n",
    "This project implemented a Handwriting Recognition system using:\n",
    "- **OpenCV** for image preprocessing and text detection\n",
    "- **CRNN (CNN + BiLSTM)** architecture for sequence recognition\n",
    "- **CTC Loss** for handling variable-length text alignment\n",
    "\n",
    "---\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "| Aspect | Finding |\n",
    "|--------|---------|\n",
    "| Dataset Size | 400,000+ handwritten names |\n",
    "| Data Quality | ~5-10% entries marked as UNREADABLE |\n",
    "| Label Distribution | Names range from 1-20+ characters, most are 4-12 chars |\n",
    "| Character Set | 67 unique characters (A-Z, a-z, 0-9, space, punctuation) |\n",
    "\n",
    "---\n",
    "\n",
    "### Hypothesis Testing Results\n",
    "\n",
    "| Hypothesis | Result |\n",
    "|------------|--------|\n",
    "| H1: DL models can achieve >85% accuracy | To be validated based on final CER/WER metrics |\n",
    "\n",
    "---\n",
    "\n",
    "### Recommendations for Improvement\n",
    "\n",
    "1. **Data Augmentation:**\n",
    "   - Add rotation, elastic deformation, and noise augmentation\n",
    "   - Use synthetic data generation for rare characters\n",
    "\n",
    "2. **Model Architecture:**\n",
    "   - Implement attention mechanisms (Transformer-based)\n",
    "   - Try EfficientNet as CNN backbone\n",
    "   - Experiment with deeper LSTM layers\n",
    "\n",
    "3. **Training Strategy:**\n",
    "   - Use curriculum learning (start with shorter labels)\n",
    "   - Implement label smoothing\n",
    "   - Try mixed precision training for faster iteration\n",
    "\n",
    "4. **Production Deployment:**\n",
    "   - Optimize model with TensorFlow Lite or ONNX\n",
    "   - Implement confidence thresholds for predictions\n",
    "   - Add spell-checking post-processing\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Train on full dataset with GPU acceleration\n",
    "2. Implement data augmentation pipeline\n",
    "3. Test with Transformer-based architecture\n",
    "4. Deploy model as REST API\n",
    "5. Create user interface for real-time OCR\n",
    "\n",
    "---\n",
    "\n",
    "### References\n",
    "\n",
    "1. Shi, B., Bai, X., & Yao, C. (2016). An End-to-End Trainable Neural Network for Image-based Sequence Recognition. TPAMI.\n",
    "2. Graves, A., et al. (2006). Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks.\n",
    "3. Kaggle Dataset: https://www.kaggle.com/datasets/landlord/handwriting-recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc243b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Project Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nDataset Overview\")\n",
    "print(f\"  Total Samples:     {len(df_train) + len(df_val) + len(df_test):,}\")\n",
    "print(f\"  Training:          {len(df_train):,}\")\n",
    "print(f\"  Validation:        {len(df_val):,}\")\n",
    "print(f\"  Test:              {len(df_test):,}\")\n",
    "\n",
    "print(\"\\nAfter Cleaning\")\n",
    "print(f\"  Training:          {len(df_train_clean):,}\")\n",
    "print(f\"  Validation:        {len(df_val_clean):,}\")\n",
    "print(f\"  Test:              {len(df_test_clean):,}\")\n",
    "\n",
    "print(\"\\nModel Configuration\")\n",
    "print(f\"  Architecture:      CRNN (CNN + Bidirectional LSTM)\")\n",
    "print(f\"  Image Size:        {IMG_WIDTH} x {IMG_HEIGHT}\")\n",
    "print(f\"  Vocabulary:        {VOCAB_SIZE} characters\")\n",
    "print(f\"  Max Label Length:  {MAX_LABEL_LENGTH}\")\n",
    "\n",
    "print(\"\\nTraining Configuration\")\n",
    "print(f\"  Batch Size:        {BATCH_SIZE}\")\n",
    "print(f\"  Samples Used:      {len(df_train_sample):,}\")\n",
    "print(f\"  Optimizer:         Adam\")\n",
    "print(f\"  Loss:              CTC Loss\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f494f377",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('handwriting_recognition_model.keras')\n",
    "print(\"Model saved: handwriting_recognition_model.keras\")\n",
    "\n",
    "import json\n",
    "\n",
    "config = {\n",
    "    'img_width': IMG_WIDTH,\n",
    "    'img_height': IMG_HEIGHT,\n",
    "    'max_label_length': MAX_LABEL_LENGTH,\n",
    "    'vocab_size': VOCAB_SIZE,\n",
    "    'char_list': CHAR_LIST,\n",
    "    'char_to_num': char_to_num,\n",
    "    'num_to_char': {str(k): v for k, v in num_to_char.items()}\n",
    "}\n",
    "\n",
    "with open('model_config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "    \n",
    "print(\"Config saved: model_config.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.13.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
